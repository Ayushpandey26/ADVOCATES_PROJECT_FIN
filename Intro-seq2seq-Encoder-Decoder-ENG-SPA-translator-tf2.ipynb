{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR_iqmfjZ2Yp"
      },
      "source": [
        "# Intro to Encoder-Decoder model and the Attention mechanism\n",
        "\n",
        "# A neural machine translator from english to spanish short sentences in tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay-UdCURZ2Yr"
      },
      "source": [
        "We continue our journey through the world of NLP, in this post we are going to describe the **basic architecture of an encoder-decoder model** that we will apply to a neural *machine translation problem*, translating texts from English to Spanish. Later, we will introduce a technique that has been a great step forward in the treatment of NLP tasks: **the attention mechanism**. We will detail a basic processing of the attention applied to a scenario of a *sequence-to-sequence model*, \"many to many\" approach. But for the moment it will be a simple attention model, we will not comment on more complex models that will be discussed in future posts, when we address the subject of *Transformers*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHvBo9pcZ2Yr"
      },
      "source": [
        "### What is Neural Machine Translation?\n",
        "\n",
        "*Machine translation (MT) is the task of automatically converting source text in one language to text in another language. Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language. This makes the challenge of automatic machine translation difficult, perhaps one of the most difficult in artificial intelligence.*\n",
        "\n",
        "*Machine Learning Mastery, Jason Brownlee [1]*\n",
        "\n",
        "The initial approach to MT problems was the statistical machine translation based on the use of statistical models, probabilities, given an input sentence. Neural machine translation, or NMT for short, is the use of neural network models to learn a statistical model for machine translation.\n",
        "The key benefit to the approach is that a single system can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxMreFWDZ2Yr"
      },
      "source": [
        "### A basic approach to the Encoder-Decoder model\n",
        "\n",
        "From the above we can deduce that NMT is a problem where we process an input sequence to produce an output sequence, that is, a sequence-to-sequence (seq2seq) problem. Specifically of the many-to-many type, sequence of several elements both at the input and at the output, and the encoder-decoder architecture for recurrent neural networks is the standard method.\n",
        "\n",
        "![Alt](images/encoder_decoder_basic.png \"title Depiction of Sutskever Encoder-Decoder Model for Text Translation Taken from Sequence to Sequence Learning with Neural Networks, 2014\")\n",
        "\n",
        "The seq2seq model consists of two sub-networks, the encoder and the decoder. The encoder, on the left hand, receives sequences from the source language as inputs and produces as a result a compact representation of the input sequence, trying to summarize or condense all its information. Then that output becomes an input or initial state of the decoder, which can also receive another external input. At each time step, the decoder generates an element of its output sequence based on the input received and its current state, as well as updating its own state for the next time step.\n",
        "\n",
        "Mention that the input and output sequences are of fixed size but they do not have to match, the length of the input sequence may differ from that of the output sequence.\n",
        "\n",
        "The critical point of this model is how to get the encoder to provide the most complete and meaningful representation of its input sequence in a single output element to the decoder. Because this vector or state is the only information the decoder will receive from the input to generate the corresponding output. The longer the input, the harder to compress in a single vector.\n",
        "We will describe in detail the model and build it in a latter section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCGOVHFfZ2Yr"
      },
      "source": [
        "### Importing the libraries and initialize global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4xIE4bIOZ2Ys"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K8fdatrZZ2Ys"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UxPnG4OZ2Yt"
      },
      "source": [
        "We set the variables for data location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CvWbcXY1Z2Yt"
      },
      "outputs": [],
      "source": [
        "# Global parameters\n",
        "#root folder\n",
        "root_folder='.'\n",
        "#data_folder='.'\n",
        "data_folder_name='data'\n",
        "train_filename='spa.txt'\n",
        "\n",
        "# Variable for data directory\n",
        "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
        "train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n",
        "\n",
        "# Both train and test set are in the root data directory\n",
        "train_path = DATA_PATH\n",
        "test_path = DATA_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX7I8ybNZ2Yt"
      },
      "source": [
        "The next code cell define the parameters and hyperparameters of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r_94e7-EZ2Yt"
      },
      "outputs": [],
      "source": [
        "# Parameters for our model\n",
        "INPUT_COLUMN = 'input'\n",
        "TARGET_COLUMN = 'target'\n",
        "TARGET_FOR_INPUT = 'target_for_input'\n",
        "NUM_SAMPLES = 20000 #40000\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM=1024 #512\n",
        "\n",
        "BATCH_SIZE = 64  # Batch size for training.\n",
        "EPOCHS = 10  # Number of epochs to train for.\n",
        "\n",
        "ATTENTION_FUNC='general'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-dnGIznZ2Yt"
      },
      "source": [
        "## The dataset and text processing\n",
        "\n",
        "For this exercise we will use pairs of simple sentences, the source in English and target in Spanish, from the Tatoeba project where people contribute adding translations every day. This is the [link](http://www.manythings.org/anki/) to some traslations in different languages. There you can download the Spanish - English spa_eng.zip file, it contains 124457 pairs of sentences.\n",
        "\n",
        "The text sentences are almost clean, they are simple plain text, so we only need to remove accents, lower case the sentences and replace everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\"). The code to apply this preprocess has been taken from the Tensorflow tutorial for neural machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHgORL-gZ2Yt"
      },
      "source": [
        "### Preprocess the text data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kXhbq59BZ2Yt"
      },
      "outputs": [],
      "source": [
        "# Some function to preprocess the text data, taken from the Neural machine translation with attention tutorial\n",
        "# in Tensorflow\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    ''' Preprocess the input text w applying lowercase, removing accents,\n",
        "    creating a space between a word and the punctuation following it and\n",
        "    replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    Input:\n",
        "        - w: a string, input text\n",
        "    Output:\n",
        "        - a string, the cleaned text\n",
        "    '''\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    #w = '<start> ' + w + ' <end>'\n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JVtPyd9Z2Yt"
      },
      "source": [
        "### Loading the datasets\n",
        "\n",
        "Load the dataset into a pandas dataframe and apply the preprocess function to the input and target columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGM-ogMdZ2Yt",
        "outputId": "0bfb97d6-54dd-467a-9176-7a7565d3a27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['go .', 'go .', 'go .', 'go .', 'hi .']\n",
            "['ve . <eos>', 'vete . <eos>', 'vaya . <eos>', 'vayase . <eos>', 'hola . <eos>']\n",
            "['<sos> ve .', '<sos> vete .', '<sos> vaya .', '<sos> vayase .', '<sos> hola .']\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset: sentence in english, sentence in spanish\n",
        "df=pd.read_csv(train_filenamepath, sep=\"\\t\", header=None, names=[INPUT_COLUMN,TARGET_COLUMN], usecols=[0,1],\n",
        "               nrows=NUM_SAMPLES)\n",
        "# Preprocess the input data\n",
        "input_data=df[INPUT_COLUMN].apply(lambda x : preprocess_sentence(x)).tolist()\n",
        "# Preprocess and include the end of sentence token to the target text\n",
        "target_data=df[TARGET_COLUMN].apply(lambda x : preprocess_sentence(x)+ ' <eos>').tolist()\n",
        "# Preprocess and include a start of setence token to the input text to the decoder, it is rigth shifted\n",
        "target_input_data=df[TARGET_COLUMN].apply(lambda x : '<sos> '+ preprocess_sentence(x)).tolist()\n",
        "\n",
        "print(input_data[:5])\n",
        "print(target_data[:5])\n",
        "print(target_input_data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsp2vaWZ2Yu",
        "outputId": "a74f89ea-63cd-471c-e8c7-1acf8a84c294"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#Delete the dataframe and release the memory (if it is possible)\n",
        "del df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqSXJXEJZ2Yu"
      },
      "source": [
        "## Tokenize and process the text data\n",
        "\n",
        "Next, let's see how to prepare the data for our model. It is very simple and the steps are the following:\n",
        "- Tokenize the data, to convert the raw text into a sequence of integers. First, we create a Tokenizer object from the keras library and fit it to our text (one tokenizer for the input and another one for the output).\n",
        "- Extract sequence of integers from the text: we call the text_to_sequence method of the tokenizer for every input and output text.\n",
        "- Calculate the maximum length of the input and output sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyneD7jUZ2Yu",
        "outputId": "31c87b4a-da16-4b90-baba-cec9d1c562f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Input Length:  7\n",
            "look here .\n",
            "[62, 34, 1]\n"
          ]
        }
      ],
      "source": [
        "# Create a tokenizer for the input texts and fit it to them\n",
        "tokenizer_inputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
        "tokenizer_inputs.fit_on_texts(input_data)\n",
        "# Tokenize and transform input texts to sequence of integers\n",
        "input_sequences = tokenizer_inputs.texts_to_sequences(input_data)\n",
        "# Claculate the max length\n",
        "input_max_len = max(len(s) for s in input_sequences)\n",
        "print('Max Input Length: ', input_max_len)\n",
        "# Show some example of tokenize sentences, useful to check the tokenization\n",
        "print(input_data[1000])\n",
        "print(input_sequences[1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6VwdoU5Z2Yu"
      },
      "source": [
        "Now we repeat the steps for the output texts but now we do not want to filter special characters otherwise eos and sos token will be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le82J4jtZ2Yu",
        "outputId": "996c4cf7-2407-4bcf-8fe2-f0b937acc30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Target Length:  12\n",
            "miren aqui . <eos>\n",
            "[945, 32, 1, 2]\n",
            "<sos> miren aqui .\n",
            "[3, 945, 32, 1]\n"
          ]
        }
      ],
      "source": [
        "# tokenize the outputs\n",
        "# don't filter out special characters (filters = '')\n",
        "# otherwise <sos> and <eos> won't appear\n",
        "# By default, Kerasâ Tokenizer will trim out all the punctuations, which is not what we want.\n",
        "# we can just set filters as blank here.\n",
        "\n",
        "# Create a tokenizer for the output texts and fit it to them\n",
        "tokenizer_outputs = Tokenizer(num_words=MAX_VOCAB_SIZE, filters='')\n",
        "tokenizer_outputs.fit_on_texts(target_data)\n",
        "tokenizer_outputs.fit_on_texts(target_input_data)\n",
        "# Tokenize and transform output texts to sequence of integers\n",
        "target_sequences = tokenizer_outputs.texts_to_sequences(target_data)\n",
        "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_input_data)\n",
        "\n",
        "# determine maximum length output sequence\n",
        "target_max_len = max(len(s) for s in target_sequences)\n",
        "print('Max Target Length: ', target_max_len)\n",
        "\n",
        "print(target_data[1000])\n",
        "print(target_sequences[1000])\n",
        "print(target_input_data[1000])\n",
        "print(target_sequences_inputs[1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-BttT55Z2Yu"
      },
      "source": [
        "## Create the vocabularies\n",
        "\n",
        "Using the tokenizer we have created previously we can retrieve the vocabularies, one to match word to integer (word2idx) and a second one to match the integer to the corresponding word (idx2word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXCKaZTyZ2Yu",
        "outputId": "ccdbf8fe-997b-4fcf-9210-9b15ebdd8ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3641 unique input tokens.\n",
            "Found 7210 unique output tokens.\n"
          ]
        }
      ],
      "source": [
        "# get the word to index mapping for input language\n",
        "word2idx_inputs = tokenizer_inputs.word_index\n",
        "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
        "\n",
        "# get the word to index mapping for output language\n",
        "word2idx_outputs = tokenizer_outputs.word_index\n",
        "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
        "\n",
        "# store number of output and input words for later\n",
        "# remember to add 1 since indexing starts at 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "\n",
        "# map indexes back into real words\n",
        "# so we can view the results\n",
        "idx2word_inputs = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_outputs = {v:k for k, v in word2idx_outputs.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw-bvU5PZ2Yu"
      },
      "source": [
        "## Padding the sentences\n",
        "\n",
        "- Padding the sentences: we need to pad zeros at the end of the sequences so that all sequences have the same length. Otherwise, we won't be able train the model on batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbRpHSsnZ2Yu",
        "outputId": "6b9ef536-001f-42c8-fe91-2bcb14d2b5b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_inputs.shape: (20000, 7)\n",
            "encoder_inputs[0]: [28  1  0  0  0  0  0]\n",
            "decoder_inputs[0]: [ 3 97  1  0  0  0  0  0  0  0  0  0]\n",
            "decoder_inputs.shape: (20000, 12)\n"
          ]
        }
      ],
      "source": [
        "# pad the input sequences\n",
        "encoder_inputs = pad_sequences(input_sequences, maxlen=input_max_len, padding='post')\n",
        "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
        "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
        "# pad the decoder input sequences\n",
        "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=target_max_len, padding='post')\n",
        "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
        "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
        "# pad the target output sequences\n",
        "decoder_targets = pad_sequences(target_sequences, maxlen=target_max_len, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TbqV3lIZ2Yu"
      },
      "source": [
        "### Create the batch data generator\n",
        "\n",
        "- Create a batch data generator: we want to train the model on batches, group of sentences, so we need to create a Dataset using the tf.data library and the function batch_on_slices on the input and output sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5PxUMomFZ2Yu"
      },
      "outputs": [],
      "source": [
        "# Define a dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (encoder_inputs, decoder_inputs, decoder_targets))\n",
        "dataset = dataset.shuffle(len(input_data)).batch(\n",
        "    BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKpW93EiZ2Yu"
      },
      "source": [
        "## Build an Encoder-Decoder model with Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2tY-QVSZ2Yu"
      },
      "source": [
        "For a better understanding, we can divide the model in three basic components:\n",
        "\n",
        "![Alt](images/encoder_decoder_RNN.jpeg \"From Understanding Encoder-Decoder Sequence to Sequence Model by Simeon Kostadinov [3]\")\n",
        "\n",
        "- The **encoder**: Layers of recurrent units where in each time step, receive a an input token, collects relevant information and produce a hidden state. Depends on the type of RNN, in our example a LSTM, the unit \"mixes\" the current hidden state and the input and return an output, discarded, and a new hidden state. You can read my postÂ â¦ for more information.\n",
        "\n",
        "\n",
        "- The **encoder vector**: it is the last hidden state of the encoder and it tries to contain as much of the useful input information as possible to help the decoder get the best results. It is only information from the input that the decoder will get.\n",
        "\n",
        "\n",
        "- The **decoder**: Layers of recurrent units, i.e. LSTMs, where each unit produces an output at a time step t. The hidden state of the first unit is the encoder vector and the rest of units accept the hidden state from the previous unit. The output is calculated using a softmax function to obtain a  probability for every token in the output vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj7Pto61Z2Yu"
      },
      "source": [
        "### Encoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MEbDHi_SZ2Yu"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Define the embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define the RNN layer, LSTM\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, input_sequence, states):\n",
        "        # Embed the input\n",
        "        embed = self.embedding(input_sequence)\n",
        "        # Call the LSTM unit\n",
        "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def init_states(self, batch_size):\n",
        "        # Return a all 0s initial states\n",
        "        return (tf.zeros([batch_size, self.hidden_dim]),\n",
        "                tf.zeros([batch_size, self.hidden_dim]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_al-quBZ2Yu"
      },
      "source": [
        "### Decoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gLqUWhVEZ2Yu"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # Define the embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # Define the RNN layer, LSTM\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, input_sequence, state):\n",
        "        # Embed the input\n",
        "        embed = self.embedding(input_sequence)\n",
        "        # Call the LSTM unit\n",
        "        lstm_out, state_h, state_c = self.lstm(embed, state)\n",
        "        # Dense layer to predict output token\n",
        "        logits = self.dense(lstm_out)\n",
        "\n",
        "        return logits, state_h, state_c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiK1TTHTZ2Yv"
      },
      "source": [
        "Once our encoder and decoder are defined we can init them and set the initial hidden state. We have included a simple test, calling the encoder and decoder to check they works fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDDXUhtfZ2Yv",
        "outputId": "c1e24648-32d2-4100-cd47-91ccd2a381d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 6, 1024)\n",
            "(1, 8, 7211)\n"
          ]
        }
      ],
      "source": [
        "#Set the length of the input and output vocabulary\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "#Create the encoder\n",
        "encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "# Get the initial states\n",
        "initial_state = encoder.init_states(1)\n",
        "# Call the encoder for testing\n",
        "test_encoder_output = encoder(tf.constant(\n",
        "    [[1, 23, 4, 5, 0, 0]]), initial_state)\n",
        "print(test_encoder_output[0].shape)\n",
        "# Create the decoder\n",
        "decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "# Get the initial states\n",
        "de_initial_state = test_encoder_output[1:]\n",
        "# Call the decoder for testing\n",
        "test_decoder_output = decoder(tf.constant(\n",
        "    [[1, 3, 5, 7, 9, 0, 0, 0]]), de_initial_state)\n",
        "print(test_decoder_output[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv-g4evzZ2Yv"
      },
      "source": [
        "### Create the loss function and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "warXuZAqZ2Yv"
      },
      "source": [
        "Now we need to define a custom loss function to avoid taking into account the 0 values, padding values, when calculating the loss. And also we have to define a custom accuracy function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "efNl2tCbZ2Yv"
      },
      "outputs": [],
      "source": [
        "def loss_func(targets, logits):\n",
        "    crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True)\n",
        "    # Mask padding values, they do not have to compute for loss\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    # Calculate the loss value\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    # y_pred shape is batch_size, seq length, vocab size\n",
        "    # y_true shape is batch_size, seq length\n",
        "    pred_values = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n",
        "    correct = K.cast(K.equal(y_true, pred_values), dtype='float32')\n",
        "\n",
        "    # 0 is padding, don't include those\n",
        "    mask = K.cast(K.greater(y_true, 0), dtype='float32')\n",
        "    n_correct = K.sum(mask * correct)\n",
        "    n_total = K.sum(mask)\n",
        "\n",
        "    return n_correct / n_total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYMqGxGaZ2Yv"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "As we mentioned before, we are interested in training the network in batches, therefore, we create a function that carries out the training of a batch of the data:\n",
        "- Call the encoder for the batch input sequence, the output is the encoded vector.\n",
        "- Set the decoder initial states to the encoded vector\n",
        "- Call the decoder, taking the right shifted target sequence as input. The output are the logits (the softmax function is applied in the loss function)\n",
        "- Calculate the loss and accuracy of the batch data\n",
        "- Update the learnable parameters of the encoder and the decoder\n",
        "- update the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bpnB4XiRZ2Y2"
      },
      "outputs": [],
      "source": [
        "# Use the @tf.function decorator to take advance of static graph computation\n",
        "@tf.function\n",
        "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
        "    ''' A training step, train a batch of the data and return the loss value reached\n",
        "        Input:\n",
        "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence\n",
        "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the target seq, our target sequence\n",
        "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence to the decoder, we use Teacher Forcing\n",
        "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
        "            the initial state of the encoder\n",
        "        - optimizer: a tf.keras.optimizers.\n",
        "        Output:\n",
        "        - loss: loss value\n",
        "\n",
        "    '''\n",
        "    # Networkâs computations need to be put under tf.GradientTape() to keep track of gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get the encoder outputs\n",
        "        en_outputs = encoder(input_seq, en_initial_states)\n",
        "        # Set the encoder and decoder states\n",
        "        en_states = en_outputs[1:]\n",
        "        de_states = en_states\n",
        "        # Get the encoder outputs\n",
        "        de_outputs = decoder(target_seq_in, de_states)\n",
        "        # Take the actual output\n",
        "        logits = de_outputs[0]\n",
        "        # Calculate the loss function\n",
        "        loss = loss_func(target_seq_out, logits)\n",
        "        acc = accuracy_fn(target_seq_out, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    # Calculate the gradients for the variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    # Apply the gradients and update the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfNSb5jIZ2Y2"
      },
      "source": [
        "As you can observe, our train function receives three sequences:\n",
        "\n",
        "- **Input** sequence: array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the input sequence to the encoder.\n",
        "\n",
        "- **target** sequence: array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the target of our model, the output that we want for our model.\n",
        "\n",
        "- **Target input** sequence: array of integers of shape [batch_size, max_seq_len, embedding dim]. It is the input sequence to the decoder because we use *Teacher Forcing*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i79qCzhjZ2Y2"
      },
      "source": [
        "### Teacher Forcing\n",
        "\n",
        "Teacher forcing is a training method critical to the development of deep learning models in NLP. It is a way for quickly and efficiently training recurrent neural network models that use the ground truth from a prior time step as input.\n",
        "\n",
        "\n",
        "In a recurrent network usually the input to a RNN at the time step t is the output of the RNN in the previous time step, t-1. But with teacher forcing we can use the actual output to improve the learning capabilities of the model.\n",
        "\n",
        "*\"Teacher forcing works by using the actual or expected output from the training dataset at the current time step y(t) as input in the next time step X(t+1), rather than the output generated by the network.\n",
        "So, in our example, the input to the decoder is the target sequence right-shifted, the target output at time step t is the decoder input at time step t+1.\"*\n",
        "\n",
        "When our model output do not vary from what was seen by the model during training, teacher forcing is very effective. But if we need a more \"creative\" model, where given an input sequence there can be several possible outputs, we should avoid this technique or apply it randomly (only in some random time steps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7TDuijjZ2Y2"
      },
      "source": [
        "Now, we can code the whole training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "URYvQQzvZ2Y2"
      },
      "outputs": [],
      "source": [
        "# Create the main train function\n",
        "def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer, checkpoint, checkpoint_prefix):\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        # Get the initial time\n",
        "        start = time.time()\n",
        "        # Get the initial state for the encoder\n",
        "        en_initial_states = encoder.init_states(batch_size)\n",
        "        # For every batch data\n",
        "        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "            # Train and get the loss value\n",
        "            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                # Store the loss and accuracy values\n",
        "                losses.append(loss)\n",
        "                accuracies.append(accuracy)\n",
        "                print('Epoch {} Batch {} Loss {:.4f} Acc:{:.4f}'.format(e + 1, batch, loss.numpy(), accuracy.numpy()))\n",
        "\n",
        "        # saving (checkpoint) the model every 2 epochs\n",
        "        if (e + 1) % 2 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "        print('Time taken for 1 epoch {:.4f} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    return losses, accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEvcpFFPZ2Y2"
      },
      "source": [
        "We are almost ready, our last step include a call to the main train function and we create a checkpoint object to save our model. Because the training process require a long time to run, every two epochs we save it. Later we can restore it and use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLcv18G1Z2Y2",
        "outputId": "b19ae055-5393-4857-a0ec-856f793702a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 3.8169 Acc:0.0000\n",
            "Epoch 1 Batch 100 Loss 1.7474 Acc:0.4025\n",
            "Epoch 1 Batch 200 Loss 1.7249 Acc:0.4345\n",
            "Epoch 1 Batch 300 Loss 1.5651 Acc:0.4759\n",
            "Time taken for 1 epoch 1559.2300 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.4111 Acc:0.4831\n",
            "Epoch 2 Batch 100 Loss 1.3950 Acc:0.4941\n",
            "Epoch 2 Batch 200 Loss 1.2711 Acc:0.5046\n",
            "Epoch 2 Batch 300 Loss 1.3547 Acc:0.5237\n",
            "Time taken for 1 epoch 1511.3494 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1168 Acc:0.5193\n",
            "Epoch 3 Batch 100 Loss 1.1372 Acc:0.5318\n",
            "Epoch 3 Batch 200 Loss 1.1205 Acc:0.5515\n",
            "Epoch 3 Batch 300 Loss 1.0493 Acc:0.5811\n",
            "Time taken for 1 epoch 1592.2327 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.8897 Acc:0.6178\n",
            "Epoch 4 Batch 100 Loss 0.9610 Acc:0.6024\n",
            "Epoch 4 Batch 200 Loss 0.9046 Acc:0.5930\n",
            "Epoch 4 Batch 300 Loss 0.9122 Acc:0.6046\n",
            "Time taken for 1 epoch 1535.5970 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6999 Acc:0.6399\n",
            "Epoch 5 Batch 100 Loss 0.7145 Acc:0.6407\n",
            "Epoch 5 Batch 200 Loss 0.8205 Acc:0.5971\n",
            "Epoch 5 Batch 300 Loss 0.7621 Acc:0.6616\n",
            "Time taken for 1 epoch 1591.1890 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5263 Acc:0.7273\n",
            "Epoch 6 Batch 100 Loss 0.6678 Acc:0.6676\n",
            "Epoch 6 Batch 200 Loss 0.6480 Acc:0.6526\n",
            "Epoch 6 Batch 300 Loss 0.6563 Acc:0.6538\n",
            "Time taken for 1 epoch 1580.4055 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.4548 Acc:0.7485\n",
            "Epoch 7 Batch 100 Loss 0.4660 Acc:0.7396\n",
            "Epoch 7 Batch 200 Loss 0.5364 Acc:0.6897\n"
          ]
        }
      ],
      "source": [
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "losses, accuracies = main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXQQ5aXtZ2Y2"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "When training is done, we get back the history and results, so we can explore them and plot our relevant metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaEtd-PXZ2Y2"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "# plot some data\n",
        "ax1.plot(losses, label='loss')\n",
        "#plt.plot(results.history['val_loss'], label='val_loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.legend()\n",
        "# accuracies\n",
        "ax2.plot(accuracies, label='acc')\n",
        "#plt.plot(results.history['val_accuracy_fn'], label='val_acc')\n",
        "ax2.set_title('Training Accuracy')\n",
        "ax2.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPV4ftTOZ2Y3"
      },
      "source": [
        "## Make predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYRnwc5MZ2Y3"
      },
      "source": [
        "To restore the lastest checkpoint, saved model, you can run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUYN5oSPZ2Y3"
      },
      "outputs": [],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint_dir = './training_ckpt_seq2seq'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWE31ZWmZ2Y3"
      },
      "source": [
        "In the prediction step, our input is a secuence of length one, the sos token, then we call the encoder and decoder repeatedly until we get the eos token or reach the maximum length defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncnobZUCZ2Y3"
      },
      "outputs": [],
      "source": [
        "def predict(input_text, encoder, input_max_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs):\n",
        "    if input_text is None:\n",
        "        input_text = input_data[np.random.choice(len(input_data))]\n",
        "        print(input_text)\n",
        "    # Tokenize the input sequence\n",
        "    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n",
        "    # Pad the sentence\n",
        "    input_seq = pad_sequences(input_seq, maxlen=input_max_len, padding='post')\n",
        "    print(input_seq)\n",
        "    # Set the encoder initial state\n",
        "    en_initial_states = encoder.init_states(1)\n",
        "    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n",
        "    # Create the decoder input, the sos token\n",
        "    de_input = tf.constant([[word2idx_outputs['<sos>']]])\n",
        "    # Set the decoder states to the encoder vector or encoder hidden state\n",
        "    de_state_h, de_state_c = en_outputs[1:]\n",
        "\n",
        "    out_words = []\n",
        "    while True:\n",
        "        # Decode and get the output probabilities\n",
        "        de_output, de_state_h, de_state_c = decoder(\n",
        "            de_input, (de_state_h, de_state_c))\n",
        "        # Select the word with the highest probability\n",
        "        de_input = tf.argmax(de_output, -1)\n",
        "        # Append the word to the predicted output\n",
        "        out_words.append(idx2word_outputs[de_input.numpy()[0][0]])\n",
        "        # Finish when eos token is found or the max length is reached\n",
        "        if out_words[-1] == '<eos>' or len(out_words) >= 20:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EVgqjECZ2Y3"
      },
      "source": [
        "It is time to show how our model works with some simple examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhzozQXWZ2Y3"
      },
      "outputs": [],
      "source": [
        "test_sents = [input_data[10003], input_data[10120]]\n",
        "#test_sents = [encoder_inputs[1000]]\n",
        "print(test_sents)\n",
        "for test_sent in test_sents:\n",
        "    predict(test_sent, encoder, input_max_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GvH6MMJZ2Y3"
      },
      "source": [
        "## The Attention Mechanism\n",
        "\n",
        "The previously described model based on RNNs has a serious problem when working with long sequences, because the information of the first tokens is lost or diluted as more tokens are processed. The context vector has been given the responsibility of encoding all the information in a given source sentence in to a vector of few hundred elements. it made it challenging for the models to deal with long sentences. A solution was proposed in Bahdanau et al., 2014 [4] and Luong et al., 2015,[5].\n",
        "\n",
        "\n",
        "They introduce a technique called \"Attention\", which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed, accessing to all the past hidden states of the encoder, instead of just the last one. At each decoding step, the decoder gets to look at any particular state of the encoder and can selectively pick out specific elements from that sequence to produce the output. We will focus on the Luong perspective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Qr8i1zZ2Y3"
      },
      "source": [
        "### Loung Attention layer\n",
        "\n",
        "![Alt](images/luong_attention.PNG \"Attention Mechanism by Gabriel Loye [6]\")\n",
        "\n",
        "There are two relevant points to focus on:\n",
        "\n",
        "- The **alignment vector**: is a vector with the same length that the input or source sequence and is computed at every time step of the decoder. Each of its values is the score (or the probability) of the corresponding word within the source sequence, they tell the decoder what to focus on at each time step.\n",
        "\n",
        "    There are three ways to calculate the alingment scores:\n",
        "\n",
        "    - *Dot product*: we only need to take the hidden states of the encoder and multiply them by the hidden state of the decoder\n",
        "    - *General*: very similar to dot product but a weight matrix is included.\n",
        "    - *Concat*: the decoder hidden state and encoder hidden states are added together first before being passed through a Linear layer with an tanh activation function and finally multiply by a weight matrix.\n",
        "    \n",
        "![Alt](images/formula_luong_attention.PNG \"Decoder output\")\n",
        "\n",
        "The alignment scores are softmaxed so that the weights will be between 0 to 1.\n",
        "\n",
        "\n",
        "- The context vector: It's the weighted average sum of the encoder's output, the dot product of the alignment vector and the encoder's output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5rV1DSjZ2Y3"
      },
      "outputs": [],
      "source": [
        "class LuongAttention(tf.keras.Model):\n",
        "    def __init__(self, rnn_size, attention_func):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.attention_func = attention_func\n",
        "\n",
        "        if attention_func not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(\n",
        "                'Attention score must be either dot, general or concat.')\n",
        "\n",
        "        if attention_func == 'general':\n",
        "            # General score function\n",
        "            self.wa = tf.keras.layers.Dense(rnn_size)\n",
        "        elif attention_func == 'concat':\n",
        "            # Concat score function\n",
        "            self.wa = tf.keras.layers.Dense(rnn_size, activation='tanh')\n",
        "            self.va = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, decoder_output, encoder_output):\n",
        "        if self.attention_func == 'dot':\n",
        "            # Dot score function: decoder_output (dot) encoder_output\n",
        "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "            # => score has shape: (batch_size, 1, max_len)\n",
        "            score = tf.matmul(decoder_output, encoder_output, transpose_b=True) # (batch_size, 1, max_len)\n",
        "        elif self.attention_func == 'general':\n",
        "            # General score function: decoder_output (dot) (Wa (dot) encoder_output)\n",
        "            # decoder_output has shape: (batch_size, 1, rnn_size)\n",
        "            # encoder_output has shape: (batch_size, max_len, rnn_size)\n",
        "            # => score has shape: (batch_size, 1, max_len)\n",
        "            score = tf.matmul(decoder_output, self.wa(\n",
        "                encoder_output), transpose_b=True) #(batch_size, 1, max_len)\n",
        "        elif self.attention_func == 'concat':\n",
        "            # Concat score function: va (dot) tanh(Wa (dot) concat(decoder_output + encoder_output))\n",
        "            # Decoder output must be broadcasted to encoder output's shape first\n",
        "            decoder_output = tf.tile(\n",
        "                decoder_output, [1, encoder_output.shape[1], 1]) #shape (batch size, max len,hidden_dim)\n",
        "\n",
        "            # Concat => Wa => va\n",
        "            # (batch_size, max_len, 2 * rnn_size) => (batch_size, max_len, rnn_size) => (batch_size, max_len, 1)\n",
        "            score = self.va(\n",
        "                self.wa(tf.concat((decoder_output, encoder_output), axis=-1))) # (batch_size, max len, 1)\n",
        "\n",
        "            # Transpose score vector to have the same shape as other two above\n",
        "            # (batch_size, max_len, 1) => (batch_size, 1, max_len)\n",
        "            score = tf.transpose(score, [0, 2, 1]) #(batch_size, 1, max_len)\n",
        "\n",
        "        # alignment a_t = softmax(score)\n",
        "        alignment = tf.keras.activations.softmax(score, axis=-1) #(batch_size, 1, max_len)\n",
        "\n",
        "        # context vector c_t is the weighted average sum of encoder output\n",
        "        context = tf.matmul(alignment, encoder_output) # (batch_size, 1, hidden_dim)\n",
        "\n",
        "        return context, alignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1rbwNxgZ2Y3"
      },
      "source": [
        "### Decoder with Attention\n",
        "\n",
        "Once our Attention Class has been defined, we can create the decoder. The complete sequence of steps when calling the decoder are:\n",
        "\n",
        "- Generate the encoder hidden states as usual, one for every input token\n",
        "- Apply a RNN to produce a new hidden state, taking its previous hidden state and the target output from the previous time step\n",
        "- Calculate the alignment scores as described previously\n",
        "- Calculate the context vector\n",
        "- In the last operation, the context vector is concatenated with the decoder hidden state we generated previously, then it is passed through a linear layer which acts as a classifier for us to obtain the probability scores of the next predicted word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imM_Jm7rZ2Y3"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, attention_func):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention = LuongAttention(hidden_dim, attention_func)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            hidden_dim, return_sequences=True, return_state=True)\n",
        "        self.wc = tf.keras.layers.Dense(hidden_dim, activation='tanh')\n",
        "        self.ws = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, input_sequence, state, encoder_output):\n",
        "        # Remember that the input to the decoder\n",
        "        # is now a batch of one-word sequences,\n",
        "        # which means that its shape is (batch_size, 1)\n",
        "        embed = self.embedding(input_sequence)\n",
        "\n",
        "        # Therefore, the lstm_out has shape (batch_size, 1, hidden_dim)\n",
        "        lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n",
        "\n",
        "        # Use self.attention to compute the context and alignment vectors\n",
        "        # context vector's shape: (batch_size, 1, hidden_dim)\n",
        "        # alignment vector's shape: (batch_size, 1, source_length)\n",
        "        context, alignment = self.attention(lstm_out, encoder_output)\n",
        "\n",
        "        # Combine the context vector and the LSTM output\n",
        "        # Before combined, both have shape of (batch_size, 1, hidden_dim),\n",
        "        # so let's squeeze the axis 1 first\n",
        "        # After combined, it will have shape of (batch_size, 2 * hidden_dim)\n",
        "        lstm_out = tf.concat(\n",
        "            [tf.squeeze(context, 1), tf.squeeze(lstm_out, 1)], 1)\n",
        "\n",
        "        # lstm_out now has shape (batch_size, hidden_dim)\n",
        "        lstm_out = self.wc(lstm_out)\n",
        "\n",
        "        # Finally, it is converted back to vocabulary space: (batch_size, vocab_size)\n",
        "        logits = self.ws(lstm_out)\n",
        "\n",
        "        return logits, state_h, state_c, alignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPKVB4qhZ2Y3"
      },
      "source": [
        "For testing purposes, we create a decoder and call it to check the output shapes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLRWMlwHZ2Y3"
      },
      "outputs": [],
      "source": [
        "#Set the length of the input and output vocabulary\n",
        "num_words_inputs = len(word2idx_inputs) + 1\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "#Create the encoder\n",
        "encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM, ATTENTION_FUNC)\n",
        "\n",
        "# Call the encoder and then the decoder\n",
        "initial_state = encoder.init_states(1)\n",
        "encoder_outputs = encoder(tf.constant([[1]]), initial_state)\n",
        "decoder_outputs = decoder(tf.constant(\n",
        "    [[1]]), encoder_outputs[1:], encoder_outputs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBxXJV75Z2Y3"
      },
      "source": [
        "### Train step function\n",
        "\n",
        "Now we can define our step train function, to train a batch data. It is very similar to the one we coded for the seq2seq model without attention but this time we pass all the hidden states returned by the encoder to the decoder. And we need to create a loop to iterate through the target sequences, calling the decoder for each one and calculating the loss function comparing the decoder output to the expected target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djfu7yDoZ2Y3"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):\n",
        "    ''' A training step, train a batch of the data and return the loss value reached\n",
        "        Input:\n",
        "        - input_seq: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence\n",
        "        - target_seq_out: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the target seq, our target sequence\n",
        "        - target_seq_in: array of integers, shape [batch_size, max_seq_len, embedding dim].\n",
        "            the input sequence to the decoder, we use Teacher Forcing\n",
        "        - en_initial_states: tuple of arrays of shape [batch_size, hidden_dim].\n",
        "            the initial state of the encoder\n",
        "        - optimizer: a tf.keras.optimizers.\n",
        "        Output:\n",
        "        - loss: loss value\n",
        "\n",
        "    '''\n",
        "    loss = 0.\n",
        "    acc = 0.\n",
        "    logits = None\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        en_outputs = encoder(input_seq, en_initial_states)\n",
        "        en_states = en_outputs[1:]\n",
        "        de_state_h, de_state_c = en_states\n",
        "\n",
        "        # We need to create a loop to iterate through the target sequences\n",
        "        for i in range(target_seq_out.shape[1]):\n",
        "            # Input to the decoder must have shape of (batch_size, length)\n",
        "            # so we need to expand one dimension\n",
        "            decoder_in = tf.expand_dims(target_seq_in[:, i], 1)\n",
        "            logit, de_state_h, de_state_c, _ = decoder(\n",
        "                decoder_in, (de_state_h, de_state_c), en_outputs[0])\n",
        "\n",
        "            # The loss is now accumulated through the whole batch\n",
        "            loss += loss_func(target_seq_out[:, i], logit)\n",
        "            # Store the logits to calculate the accuracy\n",
        "            logit = K.expand_dims(logit, axis=1)\n",
        "            if logits is None:\n",
        "                logits = logit\n",
        "            else:\n",
        "                logits = K.concatenate((logits,logit), axis=1)\n",
        "        # Calculate the accuracy for the batch data\n",
        "        acc = accuracy_fn(target_seq_out, logits)\n",
        "    # Update the parameters and the optimizer\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss / target_seq_out.shape[1], acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd-RLF2ZZ2Y3"
      },
      "source": [
        "### Main train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IvSB01IZ2Y3"
      },
      "outputs": [],
      "source": [
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq_att'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "losses, accuracies = main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_prefix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro1OuTlyZ2Y3"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "When training is done, we can plot the losses and accuracies obtained during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iD8a91P0Z2Y4"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
        "# plot some data\n",
        "ax1.plot(losses, label='loss')\n",
        "#plt.plot(results.history['val_loss'], label='val_loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.legend()\n",
        "# accuracies\n",
        "ax2.plot(accuracies, label='acc')\n",
        "#plt.plot(results.history['val_accuracy_fn'], label='val_acc')\n",
        "ax2.set_title('Training Accuracy')\n",
        "ax2.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPyu5JLRZ2Y4"
      },
      "source": [
        "### Prediction or inference\n",
        "\n",
        "We can restore the latest checkpoint of our model before making some predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75Y2wknkZ2Y4"
      },
      "outputs": [],
      "source": [
        "# Create an Adam optimizer and clips gradients by norm\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "# Create a checkpoint object to save the model\n",
        "checkpoint_dir = './training_ckpt_seq2seq_att'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weddDBqNZ2Y4"
      },
      "outputs": [],
      "source": [
        "def predict_seq2seq_att(input_text, input_max_len, tokenizer_inputs, word2idx_outputs, idx2word_outputs):\n",
        "    if input_text is None:\n",
        "        input_text = input_data[np.random.choice(len(input_data))]\n",
        "    print(input_text)\n",
        "    # Tokenize the input text\n",
        "    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n",
        "    # Pad the sentence\n",
        "    input_seq = pad_sequences(input_seq, maxlen=input_max_len, padding='post')\n",
        "    # Get the encoder initial states\n",
        "    en_initial_states = encoder.init_states(1)\n",
        "    # Get the encoder outputs or hidden states\n",
        "    en_outputs = encoder(tf.constant(input_seq), en_initial_states)\n",
        "    # Set the decoder input to the sos token\n",
        "    de_input = tf.constant([[word2idx_outputs['<sos>']]])\n",
        "    # Set the initial hidden states of the decoder to the hidden states of the encoder\n",
        "    de_state_h, de_state_c = en_outputs[1:]\n",
        "\n",
        "    out_words = []\n",
        "    alignments = []\n",
        "\n",
        "    while True:\n",
        "        # Get the decoder with attention output\n",
        "        de_output, de_state_h, de_state_c, alignment = decoder(\n",
        "            de_input, (de_state_h, de_state_c), en_outputs[0])\n",
        "        de_input = tf.expand_dims(tf.argmax(de_output, -1), 0)\n",
        "        # Detokenize the output\n",
        "        out_words.append(idx2word_outputs[de_input.numpy()[0][0]])\n",
        "        # Save the aligment matrix\n",
        "        alignments.append(alignment.numpy())\n",
        "\n",
        "        if out_words[-1] == '<eos>' or len(out_words) >= 20:\n",
        "            break\n",
        "    # Join the output words\n",
        "    print(' '.join(out_words))\n",
        "    return np.array(alignments), input_text.split(' '), out_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOydp-DMZ2Y4"
      },
      "source": [
        "It is time to test out model, making some predictions or doing some translation from english to spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xDPpsOLZ2Y4"
      },
      "outputs": [],
      "source": [
        "n_predictions=1\n",
        "test_sents = input_data[15005:(15005+n_predictions)]\n",
        "\n",
        "# Create the figure to plot in\n",
        "fig = plt.figure(figsize=(10, 20))\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "    # Call the predict function to get the translation\n",
        "    alignments, source, prediction = predict_seq2seq_att(test_sent, input_max_len, tokenizer_inputs,\n",
        "                                                     word2idx_outputs, idx2word_outputs)\n",
        "    attention = np.squeeze(alignments, (1, 2))\n",
        "    # Create a subplot\n",
        "    ax = fig.add_subplot(1, n_predictions, i+1)\n",
        "    ax.matshow(attention[:len(prediction), :len(source)], cmap='viridis')\n",
        "    ax.set_xticklabels([''] + source, rotation=90)\n",
        "    ax.set_yticklabels([''] + prediction)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5pmP4BEZ2Y4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4db9013"
      },
      "source": [
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!mkdir data\n",
        "!unzip spa-eng.zip -d data/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_tensorflow2_p36",
      "language": "python",
      "name": "conda_tensorflow2_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}